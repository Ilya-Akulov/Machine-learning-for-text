{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a770231",
   "metadata": {},
   "source": [
    "# Создание мешка слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41a041",
   "metadata": {},
   "source": [
    "*В этом блоке описывается создание мешка слов и поиск стоп-слова (англ. stopwords).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611882f6",
   "metadata": {},
   "source": [
    "*Для начала прочтем корпус текстов из файла \"tweets_lemm.csv\".*\n",
    "*Этот файл содержит — 5000 записей.* \n",
    "*Каждая запись содержит текст поста,лемматизированный текс и оценку его тональности. Если пост позитивный, то метка «1», если негативный — «0».*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc45c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558ff429",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tweets_lemm.csv\")\n",
    "corpus = list(data['lemm_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da21185",
   "metadata": {},
   "source": [
    "*В качестве примера данных возьмём корпус про Греку:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27dca23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'ехать Гpека чеpез pека',\n",
    "    'видеть Гpека в pека pак', \n",
    "    'сунуть Гpека pука в pека',\n",
    "    'pак за pука Гpека цап'\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af62cc4",
   "metadata": {},
   "source": [
    "*Чтобы преобразовать корпус текстов в мешок слов, обратимся к классу **CountVectorizer()** (англ. count vectorizer, «счётчик слов для создания векторов»).*\n",
    "\n",
    "*Он находится в модуле **sklearn.feature_extraction.text** (англ. «построение признаков текста»).*\n",
    "\n",
    "*Импортируем его:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24d6bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5dc513",
   "metadata": {},
   "source": [
    "*Создадим счётчик:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a87eb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fe253",
   "metadata": {},
   "source": [
    "*Передадим счётчику корпус текстов.*\n",
    "*Для этого вызовем знакомую вам функцию **fit_transform()**.*\n",
    "*Счётчик выделит из корпуса уникальные слова и посчитает количество их вхождений в каждом тексте корпуса.*\n",
    "*Отдельные буквы счётчик как слова не учитывает.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf35197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow, от англ. bag of words\n",
    "bow = count_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15f384",
   "metadata": {},
   "source": [
    "*Метод вернёт матрицу, в которой одна строка — это текст, а столбец — уникальное слово из всего корпуса.*\n",
    "*Число на их пересечении покажет, сколько раз в тексте встречалось нужное слово.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8bc2b",
   "metadata": {},
   "source": [
    "*Создадим для него мешок слов. Чтобы получить размер матрицы, посмотрим атрибут **shape**:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97721f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d80fee",
   "metadata": {},
   "source": [
    "*Всё верно, у нас 4 текста и 10 уникальных слов (предлог «в» не учитывается).*\n",
    "\n",
    "*Представим мешок слов в виде матрицы:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2de98f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 1 1 0 0 0 1]\n",
      " [1 1 0 1 1 0 0 0 0 0]\n",
      " [0 1 1 0 1 0 0 1 0 0]\n",
      " [1 0 1 0 1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aefdb7",
   "metadata": {},
   "source": [
    "*Список уникальных слов в мешке образует словарь.* \n",
    "*Он хранится в счётчике и вызывается методом **get_feature_names()** (англ. «получить имена признаков»):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4af96217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pак',\n",
       " 'pека',\n",
       " 'pука',\n",
       " 'видеть',\n",
       " 'гpека',\n",
       " 'ехать',\n",
       " 'за',\n",
       " 'сунуть',\n",
       " 'цап',\n",
       " 'чеpез']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ea202",
   "metadata": {},
   "source": [
    "**_CountVectorizer()_** *также нужен для расчёта N-грамм.*\n",
    "*Чтобы он считал словосочетания, укажем размер N-граммы через аргумент ngram_range (англ. «диапазон N-грамм»).*\n",
    "*Например, если мы ищем словосочетания по два слова в фразе, то диапазон зададим такой:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05984f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ff595",
   "metadata": {},
   "source": [
    "*Со словосочетаниями счётчик работает так же, как и со словами.*\n",
    "\n",
    "*У больших корпусов и мешки слов выходят большие, но часть слов в них может быть бессмысленной. Например, что можно сказать о тексте по местоимениям, союзам и предлогам? Чаще всего от них можно избавиться, причём тема текста и смысл предложения не изменятся. Когда мешок слов меньше и чище, проще найти слова, важные для классификации текстов.*\n",
    "\n",
    "*Чтобы почистить мешок слов, найдём стоп-слова, то есть слова без смысловой нагрузки. Их много, и для каждого языка — свои.*\n",
    "\n",
    "*Разберём пакет stopwords, который находится в модуле **nltk.corpus** библиотеки **nltk** (англ. Natural Language Toolkit, «инструментарий естественного языка»):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae92256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59669c",
   "metadata": {},
   "source": [
    "*Чтобы пакет заработал, загрузим список стоп-слов:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2e711a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iakulov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744043b5",
   "metadata": {},
   "source": [
    "*Вызовем функцию **stopwords.words()**, передадим ей аргумент **'russian'**, то есть русскоязычные стоп-слова:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d9f43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb071951",
   "metadata": {},
   "source": [
    "*При создании счётчика передадим список стоп-слов в счётчик векторов CountVectorizer():*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73c1ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=stop_words) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f294dfca",
   "metadata": {},
   "source": [
    "Теперь счётчик знает, какие слова нужно исключить из мешка слов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1a39f",
   "metadata": {},
   "source": [
    "*Проверим удаление стоп-слов на изначальном корпусе:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9108f028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = count_vect.fit_transform(corpus)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44d7bfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pак', 'pека', 'pука', 'видеть', 'гpека', 'ехать', 'сунуть', 'цап', 'чеpез']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f134ac",
   "metadata": {},
   "source": [
    "*Как видим был удален предлог 'за'*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
